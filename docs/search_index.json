[["index.html", "IRDR0047: Geospatial Data Science (2023/24) Welcome Contacts details", " IRDR0047: Geospatial Data Science (2023/24) Welcome Week Downloads Topics 6 [Slides]; [Data] Ecological Niche Modelling IMPORTANT NOTE: This GitHub page contains the guest lecture and learning materials for Week 06 (Ecological Niche Modelling) of Geospatial Data Science 2023-2024. The content has been archived and can be found here: [LINK] Contacts details Feel free to contact me via email for help, or book appointments for additional support if need be. I am based at UCL Department of Geography, North West Wing building: Name Email Room number Anwar Musah a.musah@ucl.ac.uk 115 "],["installation-of-r-and-rstudio.html", "1 Installation of R and RStudio 1.1 What is R and RStudio? 1.2 Downloading and install R and RStudio on to your laptop", " 1 Installation of R and RStudio 1.1 What is R and RStudio? R, or RStudio is a statistical software programming package that allows the user to carry out different types of statistical analysis. It can also be used as a GIS software to perform various kinds of spatial analysis. In the same vein, you can use it for data managing and geo-processing (i.e., importing different types of spatial formats for manipulation beforehand for spatial analysis). There are two versions: The famous icon on the left is the version for R, and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio). Let us talk about downloading RStudio. 1.2 Downloading and install R and RStudio on to your laptop RStudio is an open source software, and today its the go-to software for many researchers - its highly recommended for anyone in the domains of data science, scientific research, and technical communication. It is easy to access, and easy to download and install. In order for RStudio to work you must first install R. You can follow the steps to download and install the correct version of R and RStudio for your operating system (i.e., Windows or MAC). 1.2.1 Installation for MacOS users You will need to have the following software installed for R and RStudio to work properly on MAC. R (version 4.2.3) RStudio (version 2023.06.2-561) XQuartz (version 2.8.5) XCode (version 14.3.1 (14E300c)) Installation of R (4.2.3) and RStudio (2023.06.2-561) on MAC: OS User type R (Base) RStudio Desktop MAC R-4.2.3.pkg RStudio-2023.06.2-561.dmg Download the file for R-4.2.3.pkg attached in the table above. Double-click the downloaded file (i.e., R-4.2.3.pkg) and follow the steps to complete the installation. Now, download the file (i.e., .dmg) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2023.06.2-561.dmg) and then drag and drop the RStudio icon into the Applications folder to complete the installation. Installation of XQuartz (2.8.5): Some functions in R require some of the libraries from XQuartz to function properly on your MAC. You can download the latest version of XQuartz (XQuartz-2.8.5.pkg) by clicking on this LINK. You can simply complete the installation process by following its steps. Installation of XCode (14.3.1 [14E300c]): Some functions in R require some of the external developer tools from XCode application to function properly on your MAC. Go to the App Store application and get the XCode app downloaded by clicking on this LINK. Once it is downloaded, you can click on the “OPEN” button to verify it’s been downloaded. A window will prompt you to complete installation. Lastly, and for safe measures - we going to run this through our Terminal. You can open the Terminal program by going to the Applications &gt; Utilities folder and select the Terminal application In the terminal, type the following code xcode-select --install. If you get the following error message shown in the code chunk below, then it means that the XCode program has been installed properly - no need to do anything at this point. Otherwise, the terminal will proceed to install the XCode tool remotely. xcode-select: error: command line tools are already installed, use &quot;Software Update&quot; in System Settings to install updates This completes the installation process for R and RStudio on MAC. 1.2.2 Installation for Windows users You will need to have the following software installed for the rstan package to work on Windows. R (version 4.2.3) Rtools42 (version 4.2) RStudio (version 2023.06.0-421) Installation of R (4.3.2) and RStudio RStudio (2023.06.2-561) on Windows: OS User type R (Base) RStudio Desktop Windows R-4.2.3-win.exe RStudio-2023.06.2-561.exe Download the file for R-4.2.3-win.exe attached in the table above. Double-click the downloaded file (i.e., R-4.2.3-win.exe) and follow the steps to complete the installation. Now, we can download the file (i.e., .exe) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2023.06.0-421.exe) and follow the steps from the installer to complete the installation. Installation of Rtools 4.2 For Windows users, after you have completed the installation for R and RStudio, you are required to install the Rtools42 package as it contains some libraries and developer tools for the smooth functioning of R. Download the latest version of Rtools42 by clicking on this LINK to initiate the download of the Rtools42 installer. Double-click the downloaded rtools42-5355-5357.exe file and follow the steps to complete the installation. This completes the installation process for R and RStudio on Windows. This concludes the installation section and sets yor computer up for this session. "],["ecological-niche-models.html", "2 Ecological Niche Models 2.1 Introduction 2.2 Data preparation for the MAXENT analysis 2.3 MAXENT Analysis 2.4 Attributions 2.5 References (see reading list) 2.6 Data Sources 2.7 Practical homework", " 2 Ecological Niche Models 2.1 Introduction This week, we are using distributional niche models, often term as either: Ecological (or Environmental) Niche Models, or Environmental or Habitat Suitability Models. These are quantitative methods that use occurrence data in conjunction with environmental data to make a correlative model of the environmental conditions that meet an outcome’s environmental (or ecological) requirements, which in turn, can predict the relative suitability of habitat for that outcome. It has many applications in ecology, epidemiology, disaster risk reduction and social sciences (e.g., crime patterns). Today, we are going to use the Maximum Entropy Model (MAXENT) to infer zones for disaster hazards such as wildfires in California given a set of predictor variables (i.e, climate, vegetation, anthropogenic and socioeconomic risk factors which are raster). 2.1.1 Learning outcomes We are going to learn the following steps: Handling of point locations of occurrence data (i.e., points of fires) and preparing it as testing and training data; Handling of predictor variables (i.e., raster) and compiling them into a raster stack object, as well as perform extraction of raster values to the points of fire locations; Generating a Niche model using the maxent() from points, and use the stacked raster values to fit a model to estimate probability (or trigger points) of a fire hazard; Testing for model validation using ROC and AUC curves, and producing a surface based on threshold to delineate regions for suitability (in context probable trigger points for wildfires). 2.1.2 Datasets &amp; setting up the work directory Before you begin do make sure to download all data by clicking here. On your desktop, create a folder on called “IRDR0047-R” and within that folder, create another folder called “Week 6”. Make sure to extract all data from the zip folder and store it into “Week 6” folder. Open a new R script and set the work directory to Week 6’s folder. For Windows, the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/IRDR0047-R/Week 6&quot;) For MAC, the work directory will be: setwd(&quot;/Users/AccountName/Desktop/IRDR0047-R/Week 6&quot;) 2.1.3 Loading and installing packages IMPORTANT NOTE: We will need to install Java for the rjava package as well as the maxent() function and other functions associated with performing MAXENT to work. You can download the latest version of Java [HERE] Next, we will need to install the following new package(s): raster: Raster/gridded data analysis and manipulation sf: Simple Features sp: Package for providing classes for spatial data (points, lines, polygons and grids) spdep: Access to spatial functions such spsample() needed for generating background points tmap: Thematic Mapping dismo: Provides access to methods for niches distribution modelling in RStudio. rJava: Low-level interface for Java. The maxent() function depends on this so it must be installed. # Install the packages with install.package(): install.packages(&quot;dismo&quot;) install.packages(&quot;rJava&quot;) install.packages(&quot;raster&quot;) install.packages(&quot;tmap&quot;) install.packages(&quot;sf&quot;) install.packages(&quot;spdep&quot;) # Load the packages with library(): library(&quot;raster&quot;) library(&quot;dismo&quot;) library(&quot;tmap&quot;) library(&quot;sf&quot;) library(&quot;rJava&quot;) library(&quot;spdep&quot;) 2.1.4 Loading datasets We will be dealing with both point occurrence and raster data for this exercise. The point data are remote-sensed fire detection locations across the State of California during the summer period of 2018. We will be using predictor variables that are climatic (i.e., temperature, precipitation, dryness), environmental (vegetation and elevation) and other social-anthropogenic (socioeconomic deprivation) gridded raster data for California. We will combine them into a MAXENT model in order to quantify the areas that are potential trigger points for wildfires, and whether these variables greatly influencing the risk of a fire hazard. Let us begin loading the following list of raster files, each is a variable of interest: Raster: Temperature named California_temperature_summer_2018.tif Raster: Precipitation named California_precipitation_summer_2018.tif Raster: Dryness named California_dryness_summer_2018.tif Raster: Vegetation (NDVI) named California_vegetation_summer_2018.tif Raster: Elevation named California_elevation_summer_2018.tif Raster: Deprivation Index named California_socdeprivation_summer_2018.tif # load data raster data temp &lt;- raster(&quot;California_temperature_summer_2018.tif&quot;) prec &lt;- raster(&quot;California_precipitation_summer_2018.tif&quot;) dryn &lt;- raster(&quot;California_dryness_summer_2018.tif&quot;) ndvi &lt;- raster(&quot;California_vegetation_summer_2018.tif&quot;) elev &lt;- raster(&quot;California_elevation_summer_2018.tif&quot;) sevi &lt;- raster(&quot;California_socdeprivation_summer_2018.tif&quot;) Load the boundary and county shapefile for California: Shape file: California’s boundary border named .shp Shape file: California’s County borders named .shp # load shapefile data for california california_border &lt;- read_sf(&quot;California_boundary.shp&quot;) california_county &lt;- read_sf(&quot;California_county.shp&quot;) Load the point occurrence data for California: # load occurrence fire data in California california_fires &lt;- read.csv(&quot;California_Fire_Summer_2018.csv&quot;) IMPORTANT NOTES: All shape file and raster data (5km resolution) were projected to the CRS: WGS84 4236. The occurrence data imported into RStudio is a data frame object. We will need to first convert the occurrence data from data frame to a spatial points object by declaring columns longitude and latitude corresponds to x and y respectively and thus coordinates in data frame object california_fires # code chunk format coordinates(data.frame) = ~x+y coordinates(california_fires) = ~longitude+latitude Now, we need to define its coordinate reference system for the points which should be the same as any of the raster data. You can get this information by simple typing the raster object to console and printing its info there: # show details of temp as example temp We need to copy the detail under the crs section of out temp output and assign that detail to our points so as it also has a CRS of WGS84 4326. We can use the crs() function to perform this action. Now copy: +proj=longlat +datum=WGS84 +no_defs and use the following code: crs(california_fires) &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs&quot; Let us visualise the study area to examine the spatial distribution of wildfires. tm_shape(california_county) + tm_polygons() + tm_shape(california_fires) + tm_dots(col = &quot;red&quot;) Let us visualise the six predictor raster variables that will be used for the Niche modelling with MAXENT. Instead of visual each output individually - you can use tmap_arrange() to create a figure panel for the six images. # map object of temperature stored in m1 m1 &lt;- tm_shape(temp) + tm_raster(style = &quot;cont&quot;, title = &quot;Celsius&quot;, palette= &quot;Oranges&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;A&quot;) # map object of precipitation stored in m2 m2 &lt;- tm_shape(prec) + tm_raster(style = &quot;cont&quot;, title = &quot;mm&quot;, palette= &quot;Blues&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;B&quot;) # map object of dryness stored in m3 m3 &lt;- tm_shape(dryn) + tm_raster(style = &quot;cont&quot;, title = &quot;mm/0.25yr&quot;, palette= &quot;-Spectral&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;C&quot;) # map object of ndvi stored in m4 m4 &lt;- tm_shape(ndvi) + tm_raster(style = &quot;cont&quot;, title = &quot;Index&quot;, palette= &quot;Greens&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;D&quot;) # map object of elevation stored in m5 m5 &lt;- tm_shape(elev) + tm_raster(style = &quot;cont&quot;, title = &quot;m&quot;, midpoint = 1500, palette= &quot;-Spectral&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;E&quot;) # map object of sevi stored in m6 m6 &lt;- tm_shape(sevi) + tm_raster(style = &quot;cont&quot;, title = &quot;%&quot;, palette= &quot;Reds&quot;) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.position = c(&quot;right&quot;, &quot;top&quot;), title.position = c(&quot;left&quot;, &quot;bottom&quot;), title = &quot;F&quot;) # stitch the maps together using tmap_arrange() function tmap_arrange(m1, m2, m3, m4, m5, m6, nrow = 2) A: Temperature (degree Celsius); B: Precipitation (mm); C: Dryness (Evapotranspiration) (mm/0.25 year); D: Vegetation (NDVI); E: Elevation (meters [m]); &amp; F: Socioeconomic vulnerability index (%) IMPORTANT NOTES: It is good practice to produce figure outputs of the study area of interest inside the methodology section of an essay, report, dissertation, research paper etc.,. This gives you the opportunity to describe the study area that is under investigation and variables in the methods section. 2.2 Data preparation for the MAXENT analysis 2.2.1 Creating a multi-band raster using the stack() function Basically, a band is represented by a single matrix of cell values, and a raster with multiple bands contains multiple spatially coincident matrices of cell values representing the same spatial area. For example, the raster object for temp (i.e., temperature) is a single band raster object. But, if we start to stack raster objects prec, dryn, ndvi and so on top of temp, one over the other, then we have created a multi-band raster object. We need to create this multi-band raster object to enable the following action needed for the analysis: To perform the extraction of raster values from all 6 variables on to the occurrence points in one go; The entire multi-band raster object is needed for MAXENT estimation and spatial prediction We use the stack() to stack all raster grids into one object and rename the layers within the multi-band object to names that are tenable. envCovariates &lt;- stack(temp, prec, dryn, ndvi, elev, sevi) names(envCovariates) &lt;- c(&quot;Temperature&quot;, &quot;Precipitation&quot;, &quot;Dryness&quot;, &quot;NDVI&quot;, &quot;Elevation&quot;, &quot;Deprivation&quot;) 2.2.2 Preparing data for pseudo-background points as absence We need to prepare the background data. What is the background data? With Background data we are not attempting to guess point locations where an event is absent. Here, we are rather trying to characterise the environment of the study region. In this sense, background is the same, irrespective of where the point fire are found or not. Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a fire is more likely to be present than on average. In essence, we are creating a set of control points which we act as pseudo-absence. These are typically generated at random. There are several ways of performing this action using other functions such as randomPoints(), sampleRandom and many more. We are using the spsample() function because it allows the user to specify the boundary for which the background points (i.e., controls) should be randomly generated within. Twice the number of occurrence points are generated (the choice of twice is up to the user). For reproducibility in the random generation, we have set the set.seed() function to this 20000430. # set the seed set.seed(20000430) # we need to coerce &#39;sf&#39; object california_border into &#39;sp&#39; object for spsample to work california_border_sp &lt;- as(california_border, Class = &quot;Spatial&quot;) # here, spsample() generates twice number of fire occurrence points randomly within California&#39;s border background_points &lt;- spsample(california_border_sp, n=2*length(california_fires), &quot;random&quot;) 2.2.3 Extraction of all raster values from predictor variables onto presence-absence points Now, we are going to extract information from our envCovariates raster stack to both the presence and background points. This can be done using the extract() function. # perform raster extraction from the environmental covariates on to points california_fires_env &lt;- extract(envCovariates, california_fires) background_points_env &lt;- extract(envCovariates, background_points) IMPORTANT NOTES: After the extraction, the objects california_fires_env and background_points_env exist as a large matrix, and not as a data frame. For all occurrence points (i.e., presence), we need to add an indicator of 1 to signify presence; while for all background points (i.e., absence) - we need to also add an indicator of 0 to signify absence. We do this because we are modelling a probability and such niche models take outcomes that are from a Bernoulli or Binomial distribution. # convert large matrix objects to data frame objects and add outcome `fire` indicator california_fires_env &lt;-data.frame(california_fires_env,fire=1) background_points_env &lt;-data.frame(background_points_env,fire=0) # View one of the data frame head(california_fires_env, n=5) head(background_points_env, n=5) 2.2.4 Preparation of training &amp; test data for prediction &amp; model cross-validation Now, we need to complete one more step before we construct our wildfire risk model. We have to come up with a way to assess how well our model can actually predict whether we will likely find a trigger point for fires in a particular location. To make this assessment, we will need to perform some ‘cross-validation’ i.e., that is setting aside some of our presence-absence locations, and using them to test the model. In our case, we will randomly withhold 25% of the observations as test data, and retain the other 75% as training data for the prediction. This means that we will be fitting the model multiple times, withholding each fourth of the data separately, then average the results. This is called a k-fold cross-validation (in our case 4-fold). However, for our purposes of time, we will just fit the model once to demonstrate what is actually happening for you to get the gist. Ideally, we will need to perform a 4-fold cross-validation and in turn average the estimates across the values for AUC and those for the true positives and negative as described in section 5.4.2. Use the kfold() function to split the presence data from california_fires_env object into 4 equal parts. This should add an index that makes four random groups of observations. You can hold 25% of the data (i.e., the first portion) by specifying select == 1 as the test data. You can hold the remaining 75% of the data (i.e., the 2nd, 3rd and 4th portion) by specifying select != 1 as the training data. # set same set.seed() as before set.seed(20000430) # using k-fold function to split data into 4 equal parts select &lt;- kfold(california_fires_env, 4) # 25% of the fire data use for testing the model california_fires_env_test &lt;- california_fires_env[select==1,] # 75% of the fire data use for training the model california_fires_env_train &lt;- california_fires_env[select!=1,] Repeat the above process for the background points: # set same set.seed() as before set.seed(20000430) # repeat the process for the background points select &lt;- kfold(background_points_env, 4) background_points_env_test &lt;- background_points_env[select==1,] background_points_env_train &lt;- background_points_env[select!=1,] Now, let us row bind the training and test dataset together using the rbind() function: training_data &lt;- rbind(california_fires_env_train, background_points_env_train) testing_data &lt;- rbind(california_fires_env_test, background_points_env_test) We are now in the position to execute the distributional niche models. 2.3 MAXENT Analysis Now, we can fit the niche model using the Maximum Entropy (MAXENT) algorithm, which tries to define the combination of environmental risk factors that best predicts the occurrence of the wildfires in California. The maxent() allows the users to implement such algorithm. Here are some important notes on it’s usage: maxent(): This function uses environmental data for locations of known presence and for a large number of ‘background’ locations. It has three mandatory arguments - x, p, and args. x: In this argument, you must specify the columns of the predictor variables in the training data frame. The first columns in the example are the risk factors we are interested. p: In this argument, you must specify the column containing the presence and absence of fires in the training data frame. args: This allows for additional arguments. Running the maxent() code should look something like: model_training &lt;- maxent(x=training_data[,c(1:6)], p=training_data[,7], args=c(&quot;responsecurves&quot;)) 2.3.1 Examination of the predictor’s contribution and response curves The results are stored in the model_training object. We can examine which variable has the biggest contribution to the presence of wildfire presences in California: plot(model_training, pch=19, xlab = &quot;Percentage [%]&quot;, cex=1.2) IMPORTANT NOTES: We can view the contribution estimates for each covariate more precisely by typing in RConsole the following code: model_training@results. Here, we can see the following contribution estimates: NDVI (44.2321%); Elevation (23.5530%); Deprivation (12.0339%); Dryness (9.9266); Temperature (6.8892%); and Precipitation (3.3653%). The contribution estimates should sum up to 100%. Interpretation: From this plot, we can see that the model is most sensitive to variation in NDVI, followed with additional contributions from land surface elevation, and from increased levels of socioeconomic deprivation (reporting top three). We can examine as well as the likelihood of fire occurrence and how it responds to variation in these conditions. To see the shape of the response curves estimated by the model, we can use the response() function: response(model_training) Interpretation: In the response plots, we are looking at how the probability of fire occurrence (Y-axes, from zero to one) varies with each the environmental predictors (X-axes). From these plots, we can see that the MAXENT models can include complex environmental responses including plateau, linear, and nonlinear shapes, and some which are utterly unclear. For example, if we look at mean temperature during the summer, we can see that the probability for fire occurrence peaks around 0.60 when temperatures are around 30 degrees Celsius. We can also see that the probability of such outcome increases with more and more vegetation during the summer period. Probability in terms of fires in relation to deprivation is a flat line. For precipitation, dryness and elevation - the patterns are unclear. 2.3.2 Model validation An important part is model validation - this involves assessing how well does the model actually predict the occurrence of wildfires. To evaluate the predictive accuracy of the model, we turn back to our test data i.e., testing_data object, and use cross-validation to test the model. In our evaluation - there are two main outputs of interest: AUC (Area Under the Curve), which is a test of model performance where higher values indicate greater accuracy in our predictions. An AUC value of 0.5 is common cut-off point used for assessing model performance. Note that an AUC value of 0.5 or lower is the same as random guessing of presence/absence, while values towards one mean our predictions are more reliable and accurate. max TPR+TNR, which denotes the probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate. It is generally accepted that this is an optimum value at which to set the threshold for binary classification of the predicted probabilities in our mapping outputs. Anything above value is deemed as a region environmentally suitable for outcome. We use the evaluate() function to perform cross-validation analysis. # model evaluation use the test data on the trained model for validation cross_validation &lt;- evaluate(p=testing_data[testing_data$fire==1,], a=testing_data[testing_data$fire==0,], model = model_training) Here are some important notes on it’s usage: evaluate(): This function used for model evaluation and validation. It has the following arguments - p, a, and model. p: In this argument, you must specify the column of outcome and filter on the presence value e.g., testing_data[testing_data$fire==1,]. a: In this argument, you must specify the column of outcome and filter on the absence value e.g., testing_data[testing_data$fire==0,] model: Specify the full training model object e.g., model_training. Now call results and plot AUC curve: cross_validation &gt; cross_validation class : ModelEvaluation n presences : 14120 n absences : 27609 AUC : 0.9074081 cor : 0.7003824 max TPR+TNR at : 0.4054474 plot(cross_validation, &#39;ROC&#39;, cex=1.2) Interpretation: On the receiver operator curve, the 1:1 line give an AUC of 0.5. From our curve and the AUC, it is clear that our model appears to do substantially better than random guessing (high AUC value = 0.907 [90.7%]). The optimal probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate is 0.4054474 (40.55%). Hence, we will use predicted probability &gt; 0.4054 to delineate areas of suitability (or trigger points) for wildfires. 2.3.3 Mapping the predicted probability and suitability To map the predicted probabilities use the predict() function: prob_wildfire &lt;- predict(model_training, envCovariates) Generate a predicted probability map from above prob_wildfire object: # generate a publication-worthy figure # map of probability tm_shape(prob_wildfire) + tm_raster(title = &quot;Predicted probability&quot;, palette = &#39;-RdYlBu&#39;, style =&#39;cont&#39;, breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1.0))+ tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(main.title = &quot;Predicted Probability of Wild Fire [%]&quot;, main.title.position = c(0.2, 0.7), title.size=3, legend.text.size = 1.1, legend.position = c(0.65, 0.55), legend.height= -0.3, legend.title.size = 1.1, frame=&#39;white&#39;)+ tm_scale_bar(position=c(0.02, 0.02), text.size = 1, breaks = c(0, 100, 200, 300))+ tm_compass(north = 0,type = &#39;arrow&#39;, position = c(&#39;right&#39;, &#39;top&#39;), text.size = 0.9) Extract the optimal threshold value from cross validation object cross_validation using the threshold() function and use it to reclassify the raster object i.e., prob_wildfire to a binary raster. Recall, the value was 0.4054474. A probability estimate less than 0.4054474 is classed as 0 and anything above as 1. The predicted probability &gt; 0.4054 are the areas in California suitable (or expected trigger points) for wildfires. # calculate thresholds of models threshold_value &lt;- threshold(cross_validation, &quot;spec_sens&quot;) # report value threshold_value Reclassifying raster object prob_wildfire with threshold value: # prepare threshold total map create_classes_vector &lt;- c(0, threshold_value, 0, threshold_value, 1, 1) create_clasess_matrix &lt;- matrix(create_classes_vector, ncol = 3, byrow = TRUE) create_clasess_matrix &gt; create_clasess_matrix [,1] [,2] [,3] [1,] 0.0000000 0.4054474 0 [2,] 0.4054474 1.0000000 1 # create new reclassify raster based on prob_wildfires suitability_wildfires &lt;- reclassify(prob_wildfire, create_clasess_matrix) Generate final output which shows regions as trigger points: tm_shape(suitability_wildfires) + tm_raster(style = &quot;cat&quot;, title = &quot;Threshold&quot;, palette= c(&quot;lightgrey&quot;, &quot;red&quot;), labels = c(&quot;Safe&quot;, &quot;Trigger Points&quot;)) + tm_shape(california_county) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) 2.3.4 Supplementary code for 4-fold analysis Click here to see code: # split plot panel into 4 segments for 4 AUC plots par(mfrow=c(2,2)) # create a list() object to dump results inside `eMAX` eMAX&lt;-list() # use california_fires_env # use background_points_env folds &lt;- 4 kfold_pres &lt;- kfold(california_fires_env, folds) kfold_back &lt;- kfold(background_points_env, folds) set.seed(20000430) # adapting loop code from https://rpubs.com/mlibxmda/GEOG70922_Week5 # takes a long time to run 4-fold for (i in 1:folds) { train &lt;- california_fires_env[kfold_pres!= i,] test &lt;- california_fires_env[kfold_pres == i,] backTrain&lt;-background_points_env[kfold_back!=i,] backTest&lt;-background_points_env[kfold_back==i,] dataTrain&lt;-rbind(train,backTrain) dataTest&lt;-rbind(test,backTest) maxnet_eval &lt;- maxent(x=dataTrain[,c(1:6)], p=dataTrain[,7], args=c(&quot;responsecurves&quot;)) eMAX[[i]] &lt;- evaluate(p=dataTest[dataTest$fire==1,],a=dataTest[dataTest$fire==0,], maxnet_eval) plot(eMAX[[i]],&#39;ROC&#39;) } aucMAX &lt;- sapply( eMAX, function(x){slot(x, &#39;auc&#39;)} ) # report 4 of the AUC aucMAX # find the mean of AUC (and it must be &gt; 0.50) mean(aucMAX) #Get maxTPR+TNR for the maxnet model Opt_MAX&lt;-sapply( eMAX, function(x){ x@t[which.max(x@TPR + x@TNR)] } ) Opt_MAX Mean_OptMAX&lt;-mean(Opt_MAX) Mean_OptMAX # use Mean_OptMAX as threshold for mapping suitability #Note: that final results is AUC: 0.9059569; threshold: 0.4338418 2.4 Attributions This week’s practical uses content and inspiration from: Taylor, L. 2022. The social side of fire: assessing the inclusion of human social factors in fire prediction models (submitted as a dissertation [for degree in MSc Social &amp; Geographic Data Science] at UCL). Source 2.5 References (see reading list) Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Downloadable) Book: [Theory] Stockwell, D. (2019) Niche Modeling: Predictions from Statistical Distributions; Chapter 4: Topology; CRC Press; pages: 45-63. Online: [Tutorials] Hijmans, R.J., &amp; Elith, J. (2021) Species distribution modelling Click link Online: [Tutorials] Kerkhoff, D. (2016) Ecological Responses to Climate Change: Species Distribution Modelling using Maxent Click link Online: [Tutorials] Dennis, M. (2020) Practical 4: Species Distribution Modelling I Click link Paper: [Application] Escobar, L.E., (2020). Ecological Niche Modeling: An Introduction for Veterinarians and Epidemiologists, Frontiers in Veterinary Science Click link Paper: [Application] Banks, W.E., (2017). The application of ecological niche modeling methods to archaeological data in order to examine culture-environment relationships and cultural trajectories; Quarternaire Click link Paper: [Application] Liao, Y., Lei, Y., Ren, Z., Chen, H., &amp; Li., D., (2017). Predicting the potential risk area of illegal vaccine trade in China; Scientific Reports, Issue 7, 3883. Click link 2.6 Data Sources All shape files [Source: California Open Data Portal] Click Here Global Wildfires detection points [Source: Fire Information Resource Management System] Click Here Environmental data for temperature &amp; precipitation [Source: WorldClim] Click Here Socioeconomic vulnerability index (requires user login) [Source: Socioeconomic Data and Applications Center (SEDAC)] Click Here Digital Elevation Model [Source: SRTM 90m DEM Digital Elevation Database] Click Here Evapotranspiration (aridity) 1.0km [Source: NASA MODIS MOD16A2] Click Here Normalised Differenced Vegetation Index (NDVI) 250m [Source: NASA MODIS MOD13Q1] Click Here 2.7 Practical homework Suitability mapping of the Aedes mosquito and infestation in Brazil Many regions in Brazil were hit hard by the Zika virus infection outbreak in 2015. Zika infection is caused by the arboviruses transmitted by the Aedes mosquitoes which are abundant in Brazil. It is a known fact that increased abundance of the Aedes mosquito is typically associated with standing (or stagnant) water which serves as a reservoir or hotspot for breeding. Apart from the presence of standing (or stagnated) water in human dwellings, it is important to consider other intermediate factors that drive the mosquitoes to increase in population size. These factors are the following: Temperature Precipitation Population Density NDVI Land surface elevation Natural lighting Urban-rural classification The above listed variables are gridded datasets which can be downloaded by clicking [HERE]. Create a map which should the following: 1.) The predicted probability of infestation of the Aedes mosquito in Brazil; and 2.) the suitability map based on the max TPR + TNR threshold to illustrate where the Aedes mosquito will thrive in Brazil. All identified points for mosquito breeding including background points can downloaded from [HERE]. The boundaries for Brazil can be downloaded from [HERE]. The expected output should look like: Notes: For the predicted probability map (A), reclassify the predictions to the following categories - 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8 and 0.8-1.0. For the second suitability map (B), reclassify the probabilities based on the max TPR + TNR threshold i.e., below it as 0 and labelled as ‘None infested areas’ and above as 1 labelled as ‘Infested areas’. This week’s homework practical was based on the original research paper: Musah et al (2023). Coalescing disparate data sources for the geospatial prediction of mosquito abundance, using Brazil as a motivating case study. Frontiers in Tropical Diseases. Volume 4. DOI: https://doi.org/10.3389/fitd.2023.1039735 Source materials and codes for this example are hosted here: https://github.com/UCLPG-MSC-SGDS/Data-Sources "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
